{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers==4.46.2\n!pip install -q qwen-vl-utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T04:47:05.416818Z","iopub.execute_input":"2025-12-16T04:47:05.417019Z","iopub.status.idle":"2025-12-16T04:47:23.931796Z","shell.execute_reply.started":"2025-12-16T04:47:05.416993Z","shell.execute_reply":"2025-12-16T04:47:23.931091Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Download Qwen2-VL-7B-Instruct","metadata":{}},{"cell_type":"code","source":"from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import Image\nimport requests, torch\n\n# 1. Load the model (7B instruct) with FP16 on available GPUs\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\",\n    device_map=\"auto\",          # distribute across GPUs automatically\n    torch_dtype=torch.float16   # use half precision to save memory\n)\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T04:47:27.517914Z","iopub.execute_input":"2025-12-16T04:47:27.518622Z","iopub.status.idle":"2025-12-16T04:49:20.560386Z","shell.execute_reply.started":"2025-12-16T04:47:27.518577Z","shell.execute_reply":"2025-12-16T04:49:20.559547Z"}},"outputs":[{"name":"stderr","text":"2025-12-16 04:47:37.002640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765860457.202677      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765860457.255168      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f00d379ee9af4b24bb6be44a4b479ff1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf303d41997485fae0736b31dd6a17c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a13bc60ff4474295f71edfd16dc91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75cbc250add8436eab379724ad7d2edb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7554c5cba87424e922e78370807d313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de9133bb7884fbcb6b739b260896e0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f662396804d24a5d91cbae1bcc35c63f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7040b089fe44288a55022cd1c2d4f4"}},"metadata":{}},{"name":"stderr","text":"`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58abcae2c0264d128f4a739ce4c28346"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37e97dfd47f4f618e922f6c1fdba52c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3bbdceaca64740858a8a17bf7b36ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4316cdc8ab3b4f3bbb5a190ea19ccb1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcc99eb93f8b493caa3b6ddd6026e5ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cc6059edbd24062ac0255b5fcca0204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcfbb7182cb243b3a9ec5b0361e55220"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521f433ce05d43349877e0568ec7297a"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Chat function","metadata":{}},{"cell_type":"code","source":"def run(messages):\n    inputs = processor.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    )\n    \n    inputs = inputs.to(model.device)\n    \n    with torch.no_grad():\n        output_ids = model.generate(\n            inputs,\n            max_new_tokens=200,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9\n        )\n    \n    response = processor.batch_decode(\n        output_ids[:, inputs.shape[-1]:],\n        skip_special_tokens=True\n    )[0]\n    \n    print(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T04:55:45.784397Z","iopub.execute_input":"2025-12-16T04:55:45.784964Z","iopub.status.idle":"2025-12-16T04:55:45.789828Z","shell.execute_reply.started":"2025-12-16T04:55:45.784940Z","shell.execute_reply":"2025-12-16T04:55:45.789084Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Run Qwen","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Prompt dạng messages (Qwen dùng chat template)\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Explain what a transformer model is in simple terms.\"\n    }\n]\n\nrun(messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T04:54:22.923375Z","iopub.execute_input":"2025-12-16T04:54:22.923664Z","iopub.status.idle":"2025-12-16T04:54:35.290087Z","shell.execute_reply.started":"2025-12-16T04:54:22.923641Z","shell.execute_reply":"2025-12-16T04:54:35.289428Z"}},"outputs":[{"name":"stdout","text":"A transformer model is a type of artificial intelligence model that is used to process and understand natural language text. It was developed by researchers at Google in 2017 and has since become one of the most popular and widely used models for natural language processing tasks.\n\nThe basic idea behind a transformer model is to use a series of \"transformer layers\" to process the input text. Each transformer layer takes the output of the previous layer and applies a series of mathematical operations to it, including attention mechanisms that allow the model to focus on specific parts of the input text that are relevant to the task at hand.\n\nThe transformer model is particularly effective at handling long sequences of text, which makes it well-suited for tasks like language translation, text summarization, and question answering. It has also been used to achieve state-of-the-art results on a variety of other natural language processing tasks, such as sentiment analysis and text classification.\n","output_type":"stream"}],"execution_count":6}]}